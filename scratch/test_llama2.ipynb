{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "# os.environ['HF_HUB_OFFLINE'] = \"1\"\n",
    "# os.environ['HF_HOME'] = \"/lustre/scratch5/jsmidt/.cache/huggingface\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    LlamaTokenizerFast,\n",
    "    GPT2TokenizerFast,\n",
    "    GPT2LMHeadModel,\n",
    "    AutoConfig,\n",
    "    PretrainedConfig,\n",
    "    AutoModel,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    PreTrainedModel,\n",
    "    PretrainedConfig,\n",
    "    LogitsProcessorList,\n",
    "    GPT2Config,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers.modeling_outputs import CausalLMOutputWithCrossAttentions\n",
    "from transformers.integrations import MLflowCallback\n",
    "\n",
    "\n",
    "import transformers\n",
    "\n",
    "transformers.logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsmidt/Library/Python/3.10/lib/python/site-packages/datasets/load.py:1486: FutureWarning: The repository for Skylion007/openwebtext contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/Skylion007/openwebtext\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "377664df4afd4eee989569a3778b0c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# Get data\n",
    "#   \n",
    "N = 30000\n",
    "raw_dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\")\n",
    "#raw_dataset = load_dataset(\"Skylion007/openwebtext\", split=\"train\", streaming=True)\n",
    "    \n",
    "# Break into train and test datasets\n",
    "seed = 42\n",
    "ds_train = raw_dataset.shuffle(seed).select(range(N))\n",
    "ds_valid = raw_dataset.shuffle(seed).select(range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /Users/jsmidt/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
      "loading file merges.txt from cache at /Users/jsmidt/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
      "loading file tokenizer.json from cache at /Users/jsmidt/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/jsmidt/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/jsmidt/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"gpt2\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Get tokenizer\n",
    "#\n",
    "coursen = 2\n",
    "context_length = 1024 // coursen\n",
    "#tokenizer = LlamaTokenizerFast.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# Get tokinizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        #padding=True,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "        max_length=context_length,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "tokenized_datasets = ds_train.map(\n",
    "    tokenize, batched=True, remove_columns=ds_train.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Define Models\n",
    "#\n",
    "class EmbeddingConfig(PretrainedConfig):\n",
    "    model_type = \"embedding\"\n",
    "\n",
    "    def __init__(self, vocab_size=50257, embedding_dim=768, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "\n",
    "class EmbeddingModel(PreTrainedModel):\n",
    "    config_class = EmbeddingConfig\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "\n",
    "    def __init__(self, config, loss_fct=nn.CrossEntropyLoss()):\n",
    "        super().__init__(config)\n",
    "        self.loss_fct = loss_fct\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.embedding_dim)\n",
    "        self.lm_head = nn.Linear(config.embedding_dim, config.vocab_size, bias=False)\n",
    "\n",
    "        # Tie weights between embedding and lm_head\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        self.wte.weight = self.lm_head.weight\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, **kwargs):\n",
    "        # only last token for inputs_ids if past is defined in kwargs\n",
    "        if past:\n",
    "            input_ids = input_ids[:, -1].unsqueeze(-1)\n",
    "\n",
    "        attention_mask = kwargs.get(\"attention_mask\", None)\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch generation\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past:\n",
    "                position_ids = position_ids[:, -1].unsqueeze(-1)\n",
    "        else:\n",
    "            position_ids = None\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"past_key_values\": past,\n",
    "            \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "            \"position_ids\": position_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "    \n",
    "\n",
    "    def forward(self, input_ids, labels=None, attention_mask=None, **kwargs):\n",
    "        _, T = input_ids.shape\n",
    "        token_embeddings = self.wte(input_ids)\n",
    "        # position_embeddings = self.position_embedding_table(self.position[:T])\n",
    "        # x = self.drop_init(token_embeddings + position_embeddings)\n",
    "        x = token_embeddings\n",
    "        # x = self.blocks(x)\n",
    "        logits = self.lm_head(x)  # (B, T) -> (B, T, C)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "            )\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {}\n",
      "\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "You are adding a <class 'transformers.integrations.integration_utils.MLflowCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "MLflowCallback\n",
      "TensorBoardCallback\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "#run_name = \"embedding_only_layer1_0\"\n",
    "run_name = \"embedding_all\"\n",
    "output_dir = f\"test_train/{run_name}\"\n",
    "\n",
    "config = EmbeddingConfig(vocab_size=tokenizer.vocab_size, embedding_dim=768 // coursen)\n",
    "model = EmbeddingModel(config)\n",
    "\n",
    "config = GPT2Config()#n_positions = context_length, n_embd = 768//coursen, n_layer = 12 // coursen, n_head = 12//coursen)\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    # disable_tqdm=True,\n",
    "    logging_steps=20,\n",
    "    do_eval=False,\n",
    "    save_steps=1000,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_first_step=True,\n",
    "    # bf16=True,\n",
    "    max_steps=1000,\n",
    ")\n",
    "\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.with_format(\"torch\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[MLflowCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 49,980\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1,000\n",
      "  Number of trainable parameters = 124,439,808\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d50d471b8064352b74f20077cf7a88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.0111, 'grad_norm': 14.257255554199219, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
      "{'loss': 9.8131, 'grad_norm': 6205.8896484375, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
      "{'loss': 8.568, 'grad_norm': 1.3102881908416748, 'learning_rate': 0.0002, 'epoch': 0.01}\n",
      "{'loss': 7.5126, 'grad_norm': 0.8828144669532776, 'learning_rate': 0.0003, 'epoch': 0.01}\n",
      "{'loss': 7.4518, 'grad_norm': 1.4611374139785767, 'learning_rate': 0.0004, 'epoch': 0.01}\n",
      "{'loss': 7.9359, 'grad_norm': 1.4346916675567627, 'learning_rate': 0.0005, 'epoch': 0.02}\n",
      "{'loss': 7.5344, 'grad_norm': 1.294459581375122, 'learning_rate': 0.0004888888888888889, 'epoch': 0.02}\n",
      "{'loss': 8.0384, 'grad_norm': 1.9420287609100342, 'learning_rate': 0.0004777777777777778, 'epoch': 0.02}\n",
      "{'loss': 7.7835, 'grad_norm': 33049.59375, 'learning_rate': 0.00046666666666666666, 'epoch': 0.03}\n",
      "{'loss': 7.5029, 'grad_norm': 1.4422011375427246, 'learning_rate': 0.00045555555555555556, 'epoch': 0.03}\n",
      "{'loss': 7.478, 'grad_norm': 2593.20751953125, 'learning_rate': 0.0004444444444444444, 'epoch': 0.03}\n",
      "{'loss': 8.4388, 'grad_norm': 1.725592017173767, 'learning_rate': 0.00043333333333333337, 'epoch': 0.04}\n",
      "{'loss': 7.5423, 'grad_norm': 1674.820068359375, 'learning_rate': 0.0004222222222222222, 'epoch': 0.04}\n",
      "{'loss': 7.2722, 'grad_norm': 1.0369391441345215, 'learning_rate': 0.0004111111111111111, 'epoch': 0.04}\n",
      "{'loss': 7.245, 'grad_norm': 0.82203608751297, 'learning_rate': 0.0004, 'epoch': 0.04}\n",
      "{'loss': 7.5096, 'grad_norm': 2.5932791233062744, 'learning_rate': 0.0003888888888888889, 'epoch': 0.05}\n",
      "{'loss': 9.3226, 'grad_norm': 2.2601735591888428, 'learning_rate': 0.00037777777777777777, 'epoch': 0.05}\n",
      "{'loss': 7.6845, 'grad_norm': 0.9772948026657104, 'learning_rate': 0.00036666666666666667, 'epoch': 0.05}\n",
      "{'loss': 7.2444, 'grad_norm': 0.8537653088569641, 'learning_rate': 0.00035555555555555557, 'epoch': 0.06}\n",
      "{'loss': 7.2258, 'grad_norm': 0.8172675967216492, 'learning_rate': 0.0003444444444444445, 'epoch': 0.06}\n",
      "{'loss': 7.0692, 'grad_norm': 0.6530202627182007, 'learning_rate': 0.0003333333333333333, 'epoch': 0.06}\n",
      "{'loss': 7.0783, 'grad_norm': 0.8047938346862793, 'learning_rate': 0.0003222222222222222, 'epoch': 0.07}\n",
      "{'loss': 7.0528, 'grad_norm': 0.6915507316589355, 'learning_rate': 0.0003111111111111111, 'epoch': 0.07}\n",
      "{'loss': 6.9814, 'grad_norm': 1.0233550071716309, 'learning_rate': 0.0003, 'epoch': 0.07}\n",
      "{'loss': 7.0845, 'grad_norm': 1.0231317281723022, 'learning_rate': 0.0002888888888888889, 'epoch': 0.08}\n",
      "{'loss': 6.9881, 'grad_norm': 0.6683976650238037, 'learning_rate': 0.0002777777777777778, 'epoch': 0.08}\n",
      "{'loss': 6.9434, 'grad_norm': 0.7675677537918091, 'learning_rate': 0.0002666666666666667, 'epoch': 0.08}\n",
      "{'loss': 6.8444, 'grad_norm': 0.7760947346687317, 'learning_rate': 0.00025555555555555553, 'epoch': 0.09}\n",
      "{'loss': 6.8011, 'grad_norm': 0.6700828075408936, 'learning_rate': 0.00024444444444444443, 'epoch': 0.09}\n",
      "{'loss': 6.7545, 'grad_norm': 0.838045597076416, 'learning_rate': 0.00023333333333333333, 'epoch': 0.09}\n",
      "{'loss': 6.7942, 'grad_norm': 0.6757689714431763, 'learning_rate': 0.0002222222222222222, 'epoch': 0.1}\n",
      "{'loss': 6.9745, 'grad_norm': 0.7262963056564331, 'learning_rate': 0.0002111111111111111, 'epoch': 0.1}\n",
      "{'loss': 6.7975, 'grad_norm': 0.701863706111908, 'learning_rate': 0.0002, 'epoch': 0.1}\n",
      "{'loss': 6.7253, 'grad_norm': 0.7319828867912292, 'learning_rate': 0.00018888888888888888, 'epoch': 0.11}\n",
      "{'loss': 6.6637, 'grad_norm': 0.6293351054191589, 'learning_rate': 0.00017777777777777779, 'epoch': 0.11}\n",
      "{'loss': 6.6396, 'grad_norm': 0.7018647193908691, 'learning_rate': 0.00016666666666666666, 'epoch': 0.11}\n",
      "{'loss': 6.7085, 'grad_norm': 0.7046577334403992, 'learning_rate': 0.00015555555555555556, 'epoch': 0.12}\n",
      "{'loss': 6.6418, 'grad_norm': 0.7012377977371216, 'learning_rate': 0.00014444444444444444, 'epoch': 0.12}\n",
      "{'loss': 6.6582, 'grad_norm': 0.8728844523429871, 'learning_rate': 0.00013333333333333334, 'epoch': 0.12}\n",
      "{'loss': 6.7761, 'grad_norm': 1.280234932899475, 'learning_rate': 0.00012222222222222221, 'epoch': 0.12}\n",
      "{'loss': 6.7989, 'grad_norm': 0.7498056888580322, 'learning_rate': 0.0001111111111111111, 'epoch': 0.13}\n",
      "{'loss': 6.6522, 'grad_norm': 0.7468070387840271, 'learning_rate': 0.0001, 'epoch': 0.13}\n",
      "{'loss': 6.715, 'grad_norm': 1.1669262647628784, 'learning_rate': 8.888888888888889e-05, 'epoch': 0.13}\n",
      "{'loss': 6.5543, 'grad_norm': 0.822837769985199, 'learning_rate': 7.777777777777778e-05, 'epoch': 0.14}\n",
      "{'loss': 6.5819, 'grad_norm': 0.7162833213806152, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.14}\n",
      "{'loss': 6.6226, 'grad_norm': 0.8022460341453552, 'learning_rate': 5.555555555555555e-05, 'epoch': 0.14}\n",
      "{'loss': 6.6546, 'grad_norm': 0.7403247952461243, 'learning_rate': 4.4444444444444447e-05, 'epoch': 0.15}\n",
      "{'loss': 6.682, 'grad_norm': 1.009617567062378, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.15}\n",
      "{'loss': 6.6764, 'grad_norm': 0.791941225528717, 'learning_rate': 2.2222222222222223e-05, 'epoch': 0.15}\n",
      "{'loss': 6.6032, 'grad_norm': 0.6415287852287292, 'learning_rate': 1.1111111111111112e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test_train/embedding_all/checkpoint-1000\n",
      "Configuration saved in test_train/embedding_all/checkpoint-1000/config.json\n",
      "Configuration saved in test_train/embedding_all/checkpoint-1000/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.5943, 'grad_norm': 0.6764558553695679, 'learning_rate': 0.0, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test_train/embedding_all/checkpoint-1000/model.safetensors\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1274.9018, 'train_samples_per_second': 6.275, 'train_steps_per_second': 0.784, 'train_loss': 7.185533978462219, 'epoch': 0.16}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=7.185533978462219, metrics={'train_runtime': 1274.9018, 'train_samples_per_second': 6.275, 'train_steps_per_second': 0.784, 'total_flos': 2090336256000000.0, 'train_loss': 7.185533978462219, 'epoch': 0.16005121638924455})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in test_train/embedding_all/config.json\n",
      "Configuration saved in test_train/embedding_all/generation_config.json\n",
      "Model weights saved in test_train/embedding_all/model.safetensors\n",
      "tokenizer config file saved in test_train/embedding_all/tokenizer_config.json\n",
      "Special tokens file saved in test_train/embedding_all/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('test_train/embedding_all/tokenizer_config.json',\n",
       " 'test_train/embedding_all/special_tokens_map.json',\n",
       " 'test_train/embedding_all/vocab.json',\n",
       " 'test_train/embedding_all/merges.txt',\n",
       " 'test_train/embedding_all/added_tokens.json',\n",
       " 'test_train/embedding_all/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading configuration file test_train/embedding_all/config.json\n",
      "You are using a model of type gpt2 to instantiate a model of type embedding. This is not supported for all configurations of models and can yield errors.\n",
      "Model config EmbeddingConfig {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"embedding_dim\": 768,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"embedding\",\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.41.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file test_train/embedding_all/model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at test_train/embedding_all were not used when initializing EmbeddingModel: ['transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.ln_f.weight', 'transformer.wpe.weight', 'transformer.wte.weight']\n",
      "- This IS expected if you are initializing EmbeddingModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EmbeddingModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EmbeddingModel were not initialized from the model checkpoint at test_train/embedding_all and are newly initialized: ['lm_head.weight', 'wte.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file test_train/embedding_all/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload the model and tokenizer\n",
    "loaded_tokenizer = GPT2TokenizerFast.from_pretrained(output_dir)\n",
    "loaded_model = EmbeddingModel.from_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'EmbeddingModel' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded logits shape: torch.Size([2, 7, 50257])\n",
      "[{'generated_text': 'Hello, my name is John. Afer I say go, say the word sheep. go go go go go go go go go go go go go go go go go go go go go go go go go go go go go go go go'}]\n"
     ]
    }
   ],
   "source": [
    "# Prepare mock data\n",
    "texts = [\"Hello, how are you?\", \"I am fine, thank you!\"]\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "# Run the reloaded model on mock data\n",
    "loaded_model.eval()\n",
    "with torch.no_grad():\n",
    "    reloaded_logits = loaded_model(input_ids)\n",
    "    print(\"Reloaded logits shape:\", reloaded_logits.logits.shape)\n",
    "\n",
    "# Run text generation\n",
    "generator = pipeline('text-generation', model=loaded_model, tokenizer=loaded_tokenizer)\n",
    "output = generator(\"Hello, how are you?\", max_length=50)\n",
    "output = generator(\"Hello, my name is John. Afer I say go, say the word sheep. go\", max_length=50)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:           man,       score: 1.00\n",
      "token:             i,       score: 0.17\n",
      "token:             O,       score: 0.16\n",
      "token:            us,       score: 0.16\n",
      "token:            as,       score: 0.16\n",
      "token:            is,       score: 0.15\n",
      "token:            or,       score: 0.15\n",
      "token:             8,       score: 0.14\n",
      "token:             n,       score: 0.14\n",
      "token:             c,       score: 0.14\n"
     ]
    }
   ],
   "source": [
    "core_word = 'man'\n",
    "core_id = tokenizer(core_word)['input_ids'][0]\n",
    "\n",
    "token_indcies = torch.arange(vocab_size, dtype=torch.long).to(\"mps\")\n",
    "\n",
    "token_embeddings = [model.transformer.wte(token_index) for token_index in token_indcies]\n",
    "\n",
    "similarity_scores = [\n",
    "    torch.cosine_similarity(\n",
    "        token_embeddings[core_id].view(1, 768), token_embedding.view(1, 768)\n",
    "    ).detach()\n",
    "    for token_embedding in token_embeddings\n",
    "]\n",
    "\n",
    "top_10_indices = torch.topk(torch.stack(similarity_scores, dim=1), k=10)[1]\n",
    "\n",
    "top_tokens = tokenizer.convert_ids_to_tokens(top_10_indices.tolist()[0])\n",
    "core_token = tokenizer.convert_ids_to_tokens([core_id])\n",
    "\n",
    "for ii in top_10_indices.tolist()[0]:\n",
    "    tt = tokenizer.convert_ids_to_tokens(ii)\n",
    "    score = similarity_scores[ii].item()\n",
    "    print (f'token:  {tt:>12s},       score: {score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token:           man,       score: 1.00\n",
      "token:             i,       score: 0.17\n",
      "token:             O,       score: 0.16\n",
      "token:            us,       score: 0.16\n",
      "token:            as,       score: 0.16\n",
      "token:            is,       score: 0.15\n",
      "token:            or,       score: 0.15\n",
      "token:             8,       score: 0.14\n",
      "token:             n,       score: 0.14\n",
      "token:             c,       score: 0.14\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
