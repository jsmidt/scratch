{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bengio 2003\n",
    "## A Neural Probabilistic Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data: Tiny Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tiny-shakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: mc\n",
      "  model_type: CHAR\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: tiny-shakespeare.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: mc.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: mc.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tiny-shakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: mw\n",
      "  model_type: WORD\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: tiny-shakespeare.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: mw.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: mw.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tiny-shakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: mu\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: tiny-shakespeare.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "unigram_model_trainer.cc(146) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(150) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(201) LOG(INFO) Initialized 33662 seed sentencepieces\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 25670\n",
      "unigram_model_trainer.cc(491) LOG(INFO) Using 25670 sentences for EM training\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=12635 obj=11.7318 num_tokens=52726 num_tokens/piece=4.17301\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=10764 obj=9.49501 num_tokens=53076 num_tokens/piece=4.93088\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=8070 obj=9.5054 num_tokens=56303 num_tokens/piece=6.97683\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=8070 obj=9.46564 num_tokens=56387 num_tokens/piece=6.98724\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=6051 obj=9.65189 num_tokens=61609 num_tokens/piece=10.1816\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=6050 obj=9.6088 num_tokens=61601 num_tokens/piece=10.182\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=4537 obj=9.86628 num_tokens=67860 num_tokens/piece=14.957\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=4537 obj=9.8169 num_tokens=67858 num_tokens/piece=14.9566\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=3402 obj=10.1245 num_tokens=74658 num_tokens/piece=21.9453\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=3402 obj=10.0741 num_tokens=74665 num_tokens/piece=21.9474\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=2551 obj=10.435 num_tokens=81512 num_tokens/piece=31.953\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=2551 obj=10.3807 num_tokens=81524 num_tokens/piece=31.9577\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1913 obj=10.7922 num_tokens=88786 num_tokens/piece=46.4119\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1913 obj=10.7257 num_tokens=88782 num_tokens/piece=46.4098\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1434 obj=11.2299 num_tokens=95746 num_tokens/piece=66.7685\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1434 obj=11.1547 num_tokens=95750 num_tokens/piece=66.7713\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=1075 obj=11.75 num_tokens=102667 num_tokens/piece=95.5042\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=1075 obj=11.6583 num_tokens=102681 num_tokens/piece=95.5172\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=806 obj=12.2832 num_tokens=109683 num_tokens/piece=136.083\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=806 obj=12.1675 num_tokens=109737 num_tokens/piece=136.15\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=604 obj=12.8604 num_tokens=115660 num_tokens/piece=191.49\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=604 obj=12.7409 num_tokens=115665 num_tokens/piece=191.498\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=453 obj=13.479 num_tokens=121407 num_tokens/piece=268.007\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=453 obj=13.3545 num_tokens=121421 num_tokens/piece=268.038\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=339 obj=14.1754 num_tokens=127399 num_tokens/piece=375.808\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=339 obj=14.0155 num_tokens=127404 num_tokens/piece=375.823\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=0 size=281 obj=14.7162 num_tokens=134038 num_tokens/piece=477.004\n",
      "unigram_model_trainer.cc(507) LOG(INFO) EM sub_iter=1 size=281 obj=14.5782 num_tokens=134041 num_tokens/piece=477.014\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: mu.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: mu.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: tiny-shakespeare.txt\n",
      "  input_format: \n",
      "  model_prefix: mb\n",
      "  model_type: BPE\n",
      "  vocab_size: 256\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(350) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(181) LOG(INFO) Loading corpus: tiny-shakespeare.txt\n",
      "trainer_interface.cc(406) LOG(INFO) Loaded all 32777 sentences\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(422) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(427) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(536) LOG(INFO) all chars count=1108153\n",
      "trainer_interface.cc(547) LOG(INFO) Done: 99.9692% characters are covered.\n",
      "trainer_interface.cc(557) LOG(INFO) Alphabet size=59\n",
      "trainer_interface.cc(558) LOG(INFO) Final character coverage=0.999692\n",
      "trainer_interface.cc(590) LOG(INFO) Done! preprocessed 32777 sentences.\n",
      "trainer_interface.cc(596) LOG(INFO) Tokenizing input sentences with whitespace: 32777\n",
      "trainer_interface.cc(607) LOG(INFO) Done! 25670\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=24133 min_freq=1\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=6114 size=20 all=1626 active=1566 piece=it\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=3906 size=40 all=2197 active=2137 piece=st\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=2392 size=60 all=2967 active=2907 piece=ld\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1864 size=80 all=3580 active=3520 piece=ke\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1429 size=100 all=4127 active=4067 piece=▁his\n",
      "bpe_model_trainer.cc(167) LOG(INFO) Updating active symbols. max_freq=1396 min_freq=99\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=1253 size=120 all=4692 active=1564 piece=al\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=990 size=140 all=5317 active=2189 piece=ess\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=828 size=160 all=5831 active=2703 piece=▁To\n",
      "bpe_model_trainer.cc(258) LOG(INFO) Added: freq=717 size=180 all=6251 active=3123 piece=▁are\n",
      "trainer_interface.cc(685) LOG(INFO) Saving model: mb.model\n",
      "trainer_interface.cc(697) LOG(INFO) Saving vocabs: mb.vocab\n"
     ]
    }
   ],
   "source": [
    "# Read in raw shakespeare text\n",
    "text = open('tiny-shakespeare.txt','r').read()\n",
    "\n",
    "vocab_size = 256\n",
    "# Get Tokens (Character)\n",
    "spm.SentencePieceTrainer.train(input='tiny-shakespeare.txt', vocab_size=vocab_size, model_type='char', model_prefix='mc')\n",
    "\n",
    "# Get tokens (Word)\n",
    "spm.SentencePieceTrainer.train(input='tiny-shakespeare.txt', vocab_size=vocab_size, model_type='word', model_prefix='mw')\n",
    "\n",
    "# Get tokens (Unigram)\n",
    "spm.SentencePieceTrainer.train(input='tiny-shakespeare.txt', vocab_size=vocab_size, model_type='unigram', model_prefix='mu')\n",
    "\n",
    "# Get tokens (BPE)\n",
    "spm.SentencePieceTrainer.train(input='tiny-shakespeare.txt', vocab_size=vocab_size, model_type='bpe', model_prefix='mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 51, 12, 10, 9, 5, 3, 39, 12, 5, 12, 59, 4, 11, 26, 3, 45, 4, 20, 6, 10, 4, 3, 19, 4, 3, 25, 10, 6, 21, 4, 4, 14, 3, 7, 11, 17, 3, 20, 15, 10, 5, 8, 4, 10, 18, 3, 8, 4, 7, 10, 3, 16, 4, 3, 9, 25, 4, 7, 30, 27, 3, 28, 13, 13, 26, 3, 37, 25, 4, 7, 30, 18, 3, 9, 25, 4, 7, 30, 27, 3, 51, 12, 10, 9, 5, 3, 39, 12, 5, 12, 59, 4, 11, 26, 3, 52, 6, 15]\n",
      "Character size:  99\n",
      "\n",
      "[111, 247, 0, 40, 0, 144, 0, 149, 25, 0, 111, 247, 76]\n",
      "Word size 13\n",
      "\n",
      "[3, 137, 91, 25, 119, 90, 15, 249, 56, 11, 99, 4, 158, 4, 92, 195, 68, 67, 3, 44, 13, 42, 60, 150, 6, 50, 21, 65, 241, 16, 75, 57, 11, 82, 20, 4, 10, 49, 6, 241, 16, 3, 137, 91, 25, 119, 90, 15, 249, 56, 11, 190]\n",
      "Unigram size 52\n",
      "\n",
      "[103, 63, 42, 83, 22, 206, 253, 30, 220, 67, 198, 214, 128, 84, 37, 129, 59, 65, 166, 211, 18, 120, 199, 90, 212, 41, 32, 68, 172, 145, 224, 221, 40, 21, 220, 70, 219, 145, 224, 212, 172, 145, 224, 221, 103, 63, 42, 83, 22, 206, 253, 30, 220, 138, 6]\n",
      "BPE size 55\n"
     ]
    }
   ],
   "source": [
    "# Language Model (Character)\n",
    "spc = spm.SentencePieceProcessor('mc.model')\n",
    "\n",
    "# Language Model (Word)\n",
    "spw = spm.SentencePieceProcessor('mw.model')\n",
    "\n",
    "# Language Model (Unigram)\n",
    "spu = spm.SentencePieceProcessor('mu.model')\n",
    "\n",
    "# Language Model (BPE)\n",
    "spb = spm.SentencePieceProcessor('mb.model')\n",
    "\n",
    "\n",
    "# Show text\n",
    "print (spc.Encode(text[:100]))\n",
    "print ('Character size: ', len(spc.Encode(text[:100])))\n",
    "print ()\n",
    "print (spw.Encode(text[:100]))\n",
    "print ('Word size', len(spw.Encode(text[:100])))\n",
    "print ()\n",
    "print (spu.Encode(text[:100]))\n",
    "print ('Unigram size', len(spu.Encode(text[:100])))\n",
    "print ()\n",
    "print (spb.Encode(text[:100]))\n",
    "print ('BPE size', len(spb.Encode(text[:100])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[226, 208,  45, 177,  53, 123, 220,  38],\n",
      "        [130, 200, 212,  80,  13, 205,  84,   7],\n",
      "        [ 10, 203, 210,  49,  25, 101,  35, 152],\n",
      "        [ 98, 201, 218, 218,  47,  98, 198, 198]])\n",
      "tensor([[208,  45, 177,  53, 123, 220,  38, 204],\n",
      "        [200, 212,  80,  13, 205,  84,   7, 202],\n",
      "        [203, 210,  49,  25, 101,  35, 152, 198],\n",
      "        [201, 218, 218,  47,  98, 198, 198, 207]])\n",
      "\n",
      "x =    'd and shall be so: T // y =     d and shall be so: Tr\n",
      "x =        No, for then we s // y =         o, for then we sh\n",
      "x =        insman come to se // y =          sman come to see\n",
      "x =            stabbing stee // y =              abbing steel\n"
     ]
    }
   ],
   "source": [
    "class Shakepeare(Dataset):\n",
    "    def __init__(self, text, block_size) -> None:\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.block_size = block_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #ix = torch.randint(self.randlen)\n",
    "        x = self.text[index:index+self.block_size]\n",
    "        y = self.text[index + 1:index+self.block_size + 1]\n",
    "        return x, y\n",
    "\n",
    "data = Shakepeare(torch.tensor(spb.Encode(text)), block_size=8)\n",
    "data_dl = DataLoader(data, batch_size=4, shuffle=True)\n",
    "\n",
    "for batch in data_dl:\n",
    "    x, y = batch \n",
    "    print (x)\n",
    "    print (y)\n",
    "    print ()\n",
    "    dx = spb.Decode(x.tolist())\n",
    "    dy = spb.Decode(y.tolist())\n",
    "\n",
    "    for wx, wy in zip(dx, dy):\n",
    "        print (f'x ={wx:>25s} // y = {wy:>25s}')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talk of pride', 'go with him; And go', 'n. JULIE', 't! By and by. I']\n",
      "['alk of pride:', 'o with him; And go,', '. JULIET', '! By and by. I h']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spb\u001b[39m.\u001b[39mDecode(x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "spb.Decode(x[0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my foe. FRI', 'Come, lords, a', 'l iron arms', 'framed, but force']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spb.Decode(x.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
