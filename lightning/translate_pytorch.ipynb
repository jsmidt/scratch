{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, DataLoader2\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torchtext\n",
    "import  torchdata\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr text length is:  23824555\n",
      "en text length is:  21392513\n",
      "david gallo: voici bill lange. je suis dave gallo.\n",
      "\n",
      "david gallo: this is bill lange. i'm dave gallo.\n",
      "\n",
      "[4135, 74, 720, 16333, 16361, 475, 3148, 7, 1044, 16344, 77, 311, 13520, 74, 720, 16333, 16344]\n",
      "[3608, 3998, 16343, 16374, 69, 58, 1882, 36, 1260, 16361, 7, 16364, 16353, 10871, 3998, 16343, 16361]\n",
      "3\n",
      "<pad>\n",
      "3\n",
      "<pad>\n",
      "david gallo: voici bill lange. je suis dave gallo.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "david gallo: this is bill lange. i'm dave gallo.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 16384\n",
    "vtype = 'bpe'\n",
    "batch_size = 256\n",
    "lang1 = 'fr'\n",
    "lang2 = 'en'\n",
    "block_size = 32\n",
    "\n",
    "\n",
    "# Store text as pytorch datasets\n",
    "class Text(Dataset):\n",
    "    def __init__(self, text, spw_x, spw_y, block_size) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Build dataset\n",
    "        self.x = []\n",
    "        self.y = []\n",
    "        pad_x = spw_x.Encode('<pad>')[-1]\n",
    "        pad_y = spw_y.Encode('<pad>')[-1]\n",
    "        for batch in text:\n",
    "            x, y = batch\n",
    "            x = torch.tensor(spw_x.Encode(x.lower()))\n",
    "            y = torch.tensor(spw_y.Encode(y.lower()))\n",
    "\n",
    "            padl = block_size - len(x)\n",
    "            x = torch.nn.functional.pad(x, (0,padl), mode='constant', value=pad_x) \n",
    "\n",
    "            padl = block_size - len(y)\n",
    "            y = torch.nn.functional.pad(y, (0,padl), mode='constant', value=pad_y) \n",
    "\n",
    "            self.x.append(x)\n",
    "            self.y.append(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "# Get dataset for languages 1 and 2\n",
    "def get_data(vtype, batch_size, vocab_size, block_size, lang1, lang2):\n",
    "\n",
    "    raw_x = f'data/raw_{lang1}_text.txt'\n",
    "    raw_y = f'data/raw_{lang2}_text.txt'\n",
    "\n",
    "    # Build text dataset\n",
    "    train_text, valid_text, test_text = torchtext.datasets.IWSLT2016(root='data',  language_pair=(lang1, lang2))\n",
    "\n",
    "    # Concatonate dateset\n",
    "    text_x = ''\n",
    "    text_y = ''\n",
    "    for _, batch in enumerate(train_text):\n",
    "        x, y = batch\n",
    "        text_x += x\n",
    "        text_y += y\n",
    "\n",
    "    for _, batch in enumerate(valid_text):\n",
    "        x, y = batch\n",
    "        text_x += x\n",
    "        text_y += y\n",
    "\n",
    "    for _, batch in enumerate(test_text):\n",
    "        x, y = batch\n",
    "        text_x += x\n",
    "        text_y += y\n",
    "\n",
    "    print (f'{lang1} text length is: ', len(text_x))\n",
    "    print (f'{lang2} text length is: ', len(text_y))\n",
    "\n",
    "    f = open(raw_x,'w')\n",
    "    f.write(text_x.lower())\n",
    "    f.close()\n",
    "\n",
    "    f = open(raw_y,'w')\n",
    "    f.write(text_y.lower())\n",
    "    f.close()\n",
    "\n",
    "\n",
    "    spm.SentencePieceTrainer.train(input=raw_x, vocab_size=vocab_size, user_defined_symbols='<pad>',\n",
    "                                    model_type=vtype, model_prefix=f'mb_{lang1}', \n",
    "                                    minloglevel=2)\n",
    "\n",
    "    spm.SentencePieceTrainer.train(input=raw_y, vocab_size=vocab_size, user_defined_symbols='<pad>',\n",
    "                                    model_type=vtype, model_prefix=f'mb_{lang2}', \n",
    "                                    minloglevel=2)\n",
    "\n",
    "\n",
    "    spw_x = spm.SentencePieceProcessor(f'mb_{lang1}.model')\n",
    "    spw_y = spm.SentencePieceProcessor(f'mb_{lang2}.model')\n",
    "\n",
    "    x, y = next(iter(train_text))\n",
    "    print (x.lower())\n",
    "    print (y.lower())\n",
    "    print (spw_x.Encode(x.lower()))\n",
    "    print (spw_y.Encode(y.lower()))\n",
    "\n",
    "    padd = spw_x.Encode('<pad>')[-1]\n",
    "    print (padd)\n",
    "    print (spw_x.Decode([padd]))\n",
    "\n",
    "    padd = spw_y.Encode('<pad>')[-1]\n",
    "    print (padd)\n",
    "    print (spw_y.Decode([padd]))\n",
    "\n",
    "\n",
    "    train = Text(train_text, spw_x, spw_y, block_size)\n",
    "    valid = Text(valid_text, spw_x, spw_y, block_size)\n",
    "\n",
    "    x, y = next(iter(train))\n",
    "    print (spw_x.Decode(x.tolist()))\n",
    "    print (spw_y.Decode(y.tolist()))\n",
    "\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(valid, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_dl, val_dl, spw_x, spw_y\n",
    "\n",
    "train_dl, val_dl, spw_fr, spw_en = get_data(vtype, batch_size, vocab_size, block_size, lang1, lang2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  285,   680,  1443,   206,  1281,  3765,   152,   206,    39, 16343,\n",
      "         3133,  5391,    44,  1501,  3285, 16341,    39, 16343, 11716,    44,\n",
      "          734,  1522, 16344,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3])\n",
      "tensor([  148,   355,   239,   115,  1585,    43,  2123, 16365, 13784,   115,\n",
      "          225,    25,   935,  1604,  1298, 16359,  2845,     9,  1832,   544,\n",
      "        16361,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
      "            3,     3])\n",
      "ma première année -- super motivée -- j'allais enseigner le gouvernement américain, j'adorais le système politique.<pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "my first year -- super gung-ho -- going to teach american government, loved the political system.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "#batch = next(iter(train))\n",
    "#x, y = batch\n",
    "#print (spw_fr.Decode(x.tolist()))\n",
    "#print (spw_en.Decode(y.tolist()))\n",
    "\n",
    "#train_dl = DataLoader2(train, batch_size=128, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dl))\n",
    "x, y = batch\n",
    "print (x[0])\n",
    "print (y[0])\n",
    "print (spw_fr.Decode(x[0].tolist()))\n",
    "print (spw_en.Decode(y[0].tolist()))\n",
    "\n",
    "#lenn = []\n",
    "#for i in range(10000):\n",
    "#    lenn.append(len(train[i][0]))#\n",
    "\n",
    "#lenn = torch.tensor(lenn).float()\n",
    "#print (lenn.median(), lenn.median() + lenn.std(), lenn.min(), lenn.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "print (spw_fr.Decode([16344]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([25])\n",
      "tensor([25,  3,  3,  3,  3,  3,  3,  3])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "26.87, 46.42\n",
    "x = torch.randint(0,100,(1,))\n",
    "padl = 8 - len(x)\n",
    "y = torch.nn.functional.pad(x, (0,padl), mode='constant', value=3) \n",
    "\n",
    "print (x)\n",
    "print (y)\n",
    "print (len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "david gallo: voici bill lange. je suis dave gallo.\n",
      "david gallo: this is bill lange. i'm dave gallo.\n"
     ]
    }
   ],
   "source": [
    "fr, en = train[0]\n",
    "print (spw_fr.Decode(fr.tolist()))\n",
    "print (spw_en.Decode(en.tolist()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"David Gallo: Voici Bill Lange. Je suis Dave Gallo.\\nNous allons vous raconter quelques histoires de la mer en vidéo.\\nNous avons des vidéos du Titanic parmi les plus spectaculaires jamais vues. et nous n'allons pas vous en montrer une image.\\nLa vérité est que le Titanic -- même s'il continue de battre toutes les records de recettes -- n'est pas l'histoire la plus passionnante.\\nLe problème, je crois, est qu'on tient l'océan pour acquis.\\nQuand vous y pensez, les océans représentent 75% de la planète.\\nLa plus grande partie de la planète est d'eau.\\nLa profondeur moyenne est environ 3,2 km.\\nUne partie du problème, je pense, est qu'en étant sur la plage ou en regardant des images de l'océan, comme celles-ci, on voit cette grande étendue bleue, chatoyante, ça bouge, il y a des vagues, il y a du surf et il y a des marées, mais vous n'avez aucune idée de ce qui s'y cache.\\nIl y existe les chaînes de montagnes les plus longues de la planète.\\nLa plupart des animaux se trouvent dans les océans.\\nLa pl\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textx[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
